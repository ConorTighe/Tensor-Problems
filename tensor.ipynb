{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TesnorFlow Problem Sheet Solution:\n",
    "\n",
    "This program uses machine learning through tensor flow to identify flowers based on the IRIS dataset. First we import the dataset using pandas read_csv without headers because we already have our own ones made within the csv. We then create the feature columns for the model and set the shape as 4 since there are 4 columns our model will use. A DNNClassifier Model is created for comparing our dataset to and testing its accuacy. We then print the whole dataset to the user before using pandas to split the dataset in half. Once the dataset is split we save the training and testing sets to there own csvs. We then use the csvs to create seperate datasets and input. After testing these we compare the test to our model and print the accuracy. It should print out a result between 0.9-1.0. Then we use the test set to predict the type of flowers in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports:\n",
    "- Numpy: We import this library for complex math equations and for multi-dimensional arrays and matrices that the library allows us to use, these will be useful when manipulating our data.\n",
    "- Tensorflow: Import tensorflow for machine learning algorithms.\n",
    "- Pandas: This is a library for mainpulating csvs and is what we will use to seperate the data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Tensorflow to create model\n",
    "Here we identify our CSV containing our data, this is the IRIS dataset which is what we will be using later for training and testing. We import this into our program using pandas by passing it the CSV name and telling pandas not to assign headers as we all ready have them in the CSV. We then create our feature columns, these are used to describe to the Model what tensorflow will be learning and the dataset it will be using. We then create the model which will be used to assess the dataset. The model we use here is known as a Deep Neural Network Model or DNN, this is used to establish correlations between data which makes it ideal for the IRIS dataset since we will training to establish the correlation and idetifing species from a test set using our resulting correlations from the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading IRIS..\n",
      "Creating Model..\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'Model', '_tf_random_seed': 1, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_save_checkpoints_steps': None, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100}\n"
     ]
    }
   ],
   "source": [
    "# IRIS data file name set up\n",
    "IRIS = \"IRIS.csv\"\n",
    "\n",
    "# Load data from csv.\n",
    "print(\"Loading IRIS..\")\n",
    "fulldata = pd.read_csv(IRIS, header=None)\n",
    "  \n",
    "# Prepare data for modeling by checking the columns with tf.feature_column.numeric_column.\n",
    "# We have the Petal length + width and the Speal length + width so we set the shape to 4 since there are 4 columns to feature\n",
    "feature_columns = [tf.feature_column.numeric_column(\"x\", shape=[4])]\n",
    "\n",
    "# Fit our data to a Deep Neural Network Classifier Model and make a temp directory to hold it\n",
    "print(\"Creating Model..\")\n",
    "classifier = tf.estimator.DNNClassifier(feature_columns=feature_columns,\n",
    "                                          hidden_units=[10, 20, 10],\n",
    "                                          n_classes=3,\n",
    "                                          model_dir=\"Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data into training and testing\n",
    "\n",
    "First thing I do here is output the whole dataset to the screen so the user can see all the flower species, this isnt necessary but allows users to go back and compare the tensorflow results to the original dataset. I then seperate the dataset into training and testing, I use pandast to do this by telling training to take it first 100 rows of the IRIS dataset and testing to take the last 50. I stop pandas from setting index headers and then apply my own headers using the names parameter.The names format is: rows - columns - setosa - versicolor - virginica as this is how the format the DNN will use to read the data later. I then save each set to a csv from pandas without indexs and includeding the headers ive set with names. Now that the data is neatly seperated into CSVs ill use them to set up tensorflow datasets by loading them in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IRIS Dataset:\n",
      "       0    1    2    3  4\n",
      "0    6.4  2.8  5.6  2.2  2\n",
      "1    5.0  2.3  3.3  1.0  1\n",
      "2    4.9  2.5  4.5  1.7  2\n",
      "3    4.9  3.1  1.5  0.1  0\n",
      "4    5.7  3.8  1.7  0.3  0\n",
      "5    4.4  3.2  1.3  0.2  0\n",
      "6    5.4  3.4  1.5  0.4  0\n",
      "7    6.9  3.1  5.1  2.3  2\n",
      "8    6.7  3.1  4.4  1.4  1\n",
      "9    5.1  3.7  1.5  0.4  0\n",
      "10   5.2  2.7  3.9  1.4  1\n",
      "11   6.9  3.1  4.9  1.5  1\n",
      "12   5.8  4.0  1.2  0.2  0\n",
      "13   5.4  3.9  1.7  0.4  0\n",
      "14   7.7  3.8  6.7  2.2  2\n",
      "15   6.3  3.3  4.7  1.6  1\n",
      "16   6.8  3.2  5.9  2.3  2\n",
      "17   7.6  3.0  6.6  2.1  2\n",
      "18   6.4  3.2  5.3  2.3  2\n",
      "19   5.7  4.4  1.5  0.4  0\n",
      "20   6.7  3.3  5.7  2.1  2\n",
      "21   6.4  2.8  5.6  2.1  2\n",
      "22   5.4  3.9  1.3  0.4  0\n",
      "23   6.1  2.6  5.6  1.4  2\n",
      "24   7.2  3.0  5.8  1.6  2\n",
      "25   5.2  3.5  1.5  0.2  0\n",
      "26   5.8  2.6  4.0  1.2  1\n",
      "27   5.9  3.0  5.1  1.8  2\n",
      "28   5.4  3.0  4.5  1.5  1\n",
      "29   6.7  3.0  5.0  1.7  1\n",
      "..   ...  ...  ...  ... ..\n",
      "120  5.9  3.0  4.2  1.5  1\n",
      "121  6.9  3.1  5.4  2.1  2\n",
      "122  5.1  3.3  1.7  0.5  0\n",
      "123  6.0  3.4  4.5  1.6  1\n",
      "124  5.5  2.5  4.0  1.3  1\n",
      "125  6.2  2.9  4.3  1.3  1\n",
      "126  5.5  4.2  1.4  0.2  0\n",
      "127  6.3  2.8  5.1  1.5  2\n",
      "128  5.6  3.0  4.1  1.3  1\n",
      "129  6.7  2.5  5.8  1.8  2\n",
      "130  7.1  3.0  5.9  2.1  2\n",
      "131  4.3  3.0  1.1  0.1  0\n",
      "132  5.6  2.8  4.9  2.0  2\n",
      "133  5.5  2.3  4.0  1.3  1\n",
      "134  6.0  2.2  4.0  1.0  1\n",
      "135  5.1  3.5  1.4  0.2  0\n",
      "136  5.7  2.6  3.5  1.0  1\n",
      "137  4.8  3.4  1.9  0.2  0\n",
      "138  5.1  3.4  1.5  0.2  0\n",
      "139  5.7  2.5  5.0  2.0  2\n",
      "140  5.4  3.4  1.7  0.2  0\n",
      "141  5.6  3.0  4.5  1.5  1\n",
      "142  6.3  2.9  5.6  1.8  2\n",
      "143  6.3  2.5  4.9  1.5  1\n",
      "144  5.8  2.7  3.9  1.2  1\n",
      "145  6.1  3.0  4.6  1.4  1\n",
      "146  5.2  4.1  1.5  0.1  0\n",
      "147  6.7  3.1  4.7  1.5  1\n",
      "148  6.7  3.3  5.7  2.5  2\n",
      "149  6.4  2.9  4.3  1.3  1\n",
      "\n",
      "[150 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# Display the full dataset to the user and\n",
    "# Split our mixed up IRIS data into training and testing\n",
    "print(\"IRIS Dataset:\")\n",
    "print(fulldata)\n",
    "training_set = pd.read_csv(IRIS, header=None, nrows=100 , names = [\"100\", \"4\", \"setosa\", \"versicolor\", \"virginica\"])\n",
    "test_set = pd.read_csv(IRIS, header=None, skiprows=100, nrows=151, names = [\"51\", \"4\", \"setosa\", \"versicolor\", \"virginica\"])\n",
    "\n",
    "# Set the file name for both CSV\n",
    "train_data = \"trainIRIS.csv\"\n",
    "test_data = \"testIRIS.csv\"\n",
    "\n",
    "# Save them to individual CSV files\n",
    "training_set.to_csv(train_data, index = False, header=True)\n",
    "test_set.to_csv(test_data, index = False, header=True)\n",
    "\n",
    "# Create training dataset\n",
    "training_set = tf.contrib.learn.datasets.base.load_csv_with_header(\n",
    "      filename=train_data,\n",
    "      target_dtype=np.int,\n",
    "      features_dtype=np.float32)\n",
    "\n",
    "# Create testing dataset\n",
    "test = tf.contrib.learn.datasets.base.load_csv_with_header(\n",
    "      filename=test_data,\n",
    "      target_dtype=np.int,\n",
    "      features_dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "Below I set up the inputs we will be using to teach our model how to identify the flower species. I use the training set data I set up for this. Then I call the DNN model to start training using the input, It will analyse the input 2000 times for correlations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing training with inputs..\n",
      "Training model..\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from Model\\model.ckpt-162001\n",
      "INFO:tensorflow:Saving checkpoints for 162002 into Model\\model.ckpt.\n",
      "INFO:tensorflow:loss = 0.260926, step = 162002\n",
      "INFO:tensorflow:global_step/sec: 685.979\n",
      "INFO:tensorflow:loss = 0.374497, step = 162102 (0.114 sec)\n",
      "INFO:tensorflow:global_step/sec: 958.854\n",
      "INFO:tensorflow:loss = 0.289361, step = 162202 (0.106 sec)\n",
      "INFO:tensorflow:global_step/sec: 1241.24\n",
      "INFO:tensorflow:loss = 0.435002, step = 162302 (0.094 sec)\n",
      "INFO:tensorflow:global_step/sec: 862.951\n",
      "INFO:tensorflow:loss = 0.285164, step = 162402 (0.116 sec)\n",
      "INFO:tensorflow:global_step/sec: 672.287\n",
      "INFO:tensorflow:loss = 0.523058, step = 162502 (0.149 sec)\n",
      "INFO:tensorflow:global_step/sec: 549.617\n",
      "INFO:tensorflow:loss = 0.335817, step = 162602 (0.171 sec)\n",
      "INFO:tensorflow:global_step/sec: 760.131\n",
      "INFO:tensorflow:loss = 0.160661, step = 162702 (0.130 sec)\n",
      "INFO:tensorflow:global_step/sec: 987.464\n",
      "INFO:tensorflow:loss = 0.206403, step = 162802 (0.101 sec)\n",
      "INFO:tensorflow:global_step/sec: 1138.05\n",
      "INFO:tensorflow:loss = 0.41857, step = 162902 (0.085 sec)\n",
      "INFO:tensorflow:global_step/sec: 998.085\n",
      "INFO:tensorflow:loss = 0.101174, step = 163002 (0.100 sec)\n",
      "INFO:tensorflow:global_step/sec: 910.813\n",
      "INFO:tensorflow:loss = 0.541441, step = 163102 (0.115 sec)\n",
      "INFO:tensorflow:global_step/sec: 922.237\n",
      "INFO:tensorflow:loss = 0.259814, step = 163202 (0.103 sec)\n",
      "INFO:tensorflow:global_step/sec: 1026.24\n",
      "INFO:tensorflow:loss = 0.366079, step = 163302 (0.097 sec)\n",
      "INFO:tensorflow:global_step/sec: 1023.19\n",
      "INFO:tensorflow:loss = 0.443776, step = 163402 (0.101 sec)\n",
      "INFO:tensorflow:global_step/sec: 956.905\n",
      "INFO:tensorflow:loss = 0.706329, step = 163502 (0.107 sec)\n",
      "INFO:tensorflow:global_step/sec: 1019.69\n",
      "INFO:tensorflow:loss = 0.400973, step = 163602 (0.106 sec)\n",
      "INFO:tensorflow:global_step/sec: 834.083\n",
      "INFO:tensorflow:loss = 0.27758, step = 163702 (0.109 sec)\n",
      "INFO:tensorflow:global_step/sec: 1015.84\n",
      "INFO:tensorflow:loss = 0.24721, step = 163802 (0.112 sec)\n",
      "INFO:tensorflow:global_step/sec: 651.594\n",
      "INFO:tensorflow:loss = 0.19086, step = 163902 (0.141 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 164001 into Model\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.191845.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.canned.dnn.DNNClassifier at 0x22a7b585d30>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the training inputs\n",
    "print(\"Preparing training with inputs..\")\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "      x={\"x\": np.array(training_set.data)},\n",
    "      y=np.array(training_set.target),\n",
    "      num_epochs=None,\n",
    "      shuffle=True)\n",
    "\n",
    "# Train model.\n",
    "print(\"Training model..\")\n",
    "classifier.train(input_fn=train_input_fn, steps=2000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model\n",
    "\n",
    "Here we set up more inputs except this time we use the test set. We then pass these inputs to the DNN Model to test its accuacy. This should give back a accuracy higher the 0.90 if everything is working correctly. After we know how accurate out model is we can now use it for predictions. We take the same test input and pass it to our model so it can try estimate what are the species that each row of data describes. We convert the object holding the results into a list so it can be easily displayed. The reults are then printed to the screen by mapping each value so its easier for the user to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2017-11-21-18:14:13\n",
      "INFO:tensorflow:Restoring parameters from Model\\model.ckpt-164001\n",
      "INFO:tensorflow:Finished evaluation at 2017-11-21-18:14:13\n",
      "INFO:tensorflow:Saving dict for global step 164001: accuracy = 0.980392, average_loss = 0.144435, global_step = 164001, loss = 7.3662\n",
      "\n",
      "Test Accuracy: 0.980392\n",
      "\n",
      "Predicting data..\n",
      "INFO:tensorflow:Restoring parameters from Model\\model.ckpt-164001\n",
      "Prediction results:\n",
      "[[b'0'], [b'1'], [b'0'], [b'1'], [b'0'], [b'0'], [b'0'], [b'0'], [b'1'], [b'0'], [b'2'], [b'1'], [b'0'], [b'2'], [b'0'], [b'1'], [b'1'], [b'0'], [b'0'], [b'1'], [b'1'], [b'2'], [b'0'], [b'1'], [b'1'], [b'1'], [b'0'], [b'1'], [b'1'], [b'2'], [b'2'], [b'0'], [b'2'], [b'1'], [b'1'], [b'0'], [b'1'], [b'0'], [b'0'], [b'2'], [b'0'], [b'1'], [b'2'], [b'1'], [b'1'], [b'1'], [b'0'], [b'1'], [b'2'], [b'1'], [b'0']]\n"
     ]
    }
   ],
   "source": [
    "# Define the test inputs\n",
    "test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "      x={\"x\": np.array(test.data)},\n",
    "      y=np.array(test.target),\n",
    "      num_epochs=1,\n",
    "      shuffle=False)\n",
    "\n",
    "# Evaluate accuracy using test input on classifer model.\n",
    "accuracy_score = classifier.evaluate(input_fn=test_input_fn)[\"accuracy\"]\n",
    "\n",
    "# Print model accuracy\n",
    "print(\"\\nTest Accuracy: {0:f}\\n\".format(accuracy_score))\n",
    "\n",
    "# Predict the test array\n",
    "print(\"Predicting data..\")\n",
    "predict_input_fn = tf.estimator.inputs.numpy_input_fn(x={\"x\": np.array(test.data)},\n",
    "y=np.array(test.target),\n",
    "num_epochs=1,\n",
    "shuffle=False)\n",
    "\n",
    "# Get predictied classes results from output\n",
    "predictions = list(classifier.predict(input_fn=predict_input_fn))\n",
    "predicted_classes = [p[\"classes\"] for p in predictions]\n",
    "\n",
    "# Display predicted classes\n",
    "print(\"Prediction results:\")\n",
    "print('[%s]' % ', '.join(map(str, predicted_classes)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "The program gave me a 0.980392 accuracy and all the printed pridictions seem to be correct as far as I can see, this problem sheet gave me a good understanding of how to use tensorflow practically which will be helpfull in my project and drastically improved my understanding of Deep Neural Networks in general. I included a normal .py file in the repsoitory for users without jupyter notebook as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
